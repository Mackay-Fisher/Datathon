{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a76267f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4dbfe171",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('TD_HOSPITAL_TRAIN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "090f659c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [timeknown, cost, reflex, sex, blood, bloodchem1, bloodchem2, temperature, heart, psych1, glucose, psych2, psych3, bp, bloodchem3, confidence, bloodchem4, comorbidity, totalcost, breathing, age, sleep, bloodchem5, meals, pain, psych4, administratorcost, urine, diabetes, bloodchem6, education, psych5, psych6, information, death, dnr_dnr before sadm, dnr_no dnr, race_black, race_hispanic, race_other, race_white, primary_CHF, primary_COPD, primary_Cirrhosis, primary_Colon Cancer, primary_Coma, primary_Lung Cancer, primary_MOSF w/Malig, disability_Coma or Intub, disability_SIP>=30, disability_adl>=4 (>=5 if sur), disability_no(M2 and SIP pres), income_$25-$50k, income_>$50k, income_under $11k, extraprimary_COPD/CHF/Cirrhosis, extraprimary_Cancer, extraprimary_Coma, cancer_no, cancer_yes]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 60 columns]\n"
     ]
    }
   ],
   "source": [
    "# print(df['sex'])\n",
    "\n",
    "def attempt_convert_to_float(column):\n",
    "    try:\n",
    "        return column.astype(float)\n",
    "    except ValueError:  # If conversion fails, return the original column\n",
    "        return column\n",
    "\n",
    "# Apply the function to each column\n",
    "# Modify sex column without direct indexing\n",
    "df.loc[df['sex'].isin([\"male\", \"Male\", \"M\", 1]), 'sex'] = 0\n",
    "df.loc[~df['sex'].isin([0]), 'sex'] = 1\n",
    "\n",
    "# Convert to float where possible\n",
    "df = df.apply(attempt_convert_to_float)\n",
    "\n",
    "# Impute missing values for float64 columns\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == \"float64\":\n",
    "        df[col].fillna(df[col].mean(), inplace=True)\n",
    "\n",
    "# Create dummies\n",
    "df = pd.get_dummies(df, columns=['dnr', 'race', 'primary', 'disability', 'income', 'extraprimary', 'cancer'], drop_first=True)\n",
    "\n",
    "# Ensure columns are actually dropped\n",
    "df = df.drop(['dose', 'pdeath'], axis=1)\n",
    "\n",
    "nan_rows = df[df.isnull().any(axis=1)]\n",
    "print(nan_rows)\n",
    "\n",
    "# Explicit NaN check after processing\n",
    "assert not df.isna().any().any(), \"There are still NaN values in the data.\"\n",
    "\n",
    "# Splitting data and training model remains the same\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('death', axis=1)  # Features\n",
    "y = df['death']  # Target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "feature_importances = clf.feature_importances_\n",
    "features_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "perm_importance = permutation_importance(clf, X_test, y_test)\n",
    "\n",
    "perm_importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': perm_importance.importances_mean\n",
    "})\n",
    "\n",
    "top_perm_features = perm_importance_df.sort_values(by='Importance', ascending=False).head(20)\n",
    "\n",
    "top_10_features = top_perm_features['Feature'].head(10).tolist()\n",
    "\n",
    "# Create a subset of the original dataframe using these top 10 features\n",
    "test_df = df[top_10_features + ['death']]\n",
    "\n",
    "# Save to csv\n",
    "test_df.to_csv(\"test2.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "09897b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "correlation = df.corr()['death'].sort_values(ascending=False)\n",
    "# print(correlation)\n",
    "# threshold = float(input(\"Enter the threshold value you would like to analyze:\"))\n",
    "# print(threshold)\n",
    "\n",
    "threshold = 0.1\n",
    "\n",
    "# Identify features that have a correlation magnitude above the threshold\n",
    "significant_features = correlation[correlation.abs() > threshold].index.tolist()\n",
    "\n",
    "# Exclude the target variable 'death' from the features list\n",
    "significant_features.remove('death')\n",
    "\n",
    "new_df = df[significant_features + ['death']]\n",
    "\n",
    "new_df.to_csv('sig.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "74ad3272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      timeknown  bloodchem4        urine     glucose  administratorcost  \\\n",
      "0           4.0   12.000000  5360.000000  157.000000           3525.000   \n",
      "1         467.0   10.000000  2570.000000  271.000000          43200.000   \n",
      "2         533.0   28.000000  1690.000000  117.000000           5894.000   \n",
      "3          68.0   31.790697  2197.483816  159.695613          16717.000   \n",
      "4        1605.0   31.790697  2197.483816  159.695613          10151.000   \n",
      "...         ...         ...          ...         ...                ...   \n",
      "7053      841.0   12.000000  1830.000000  159.695613          43891.000   \n",
      "7054      258.0   39.000000  1130.000000  139.000000         106109.000   \n",
      "7055     1325.0   31.790697  2197.483816  159.695613          21128.000   \n",
      "7056        4.0   31.790697  2197.483816  159.695613          22312.328   \n",
      "7057       14.0   17.000000  1840.000000   96.000000           5874.000   \n",
      "\n",
      "      disability_no(M2 and SIP pres)  temperature  heart  education       age  \n",
      "0                                  0     35.59375  103.0  20.000000  76.56396  \n",
      "1                                  0     39.00000   50.0  16.000000  63.33499  \n",
      "2                                  1     38.19531   50.0   5.000000  70.52698  \n",
      "3                                  0     37.59375   80.0  12.000000  55.31799  \n",
      "4                                  1     35.69531  114.0   2.000000  67.06598  \n",
      "...                              ...          ...    ...        ...       ...  \n",
      "7053                               0     38.00000  150.0  17.000000  88.91199  \n",
      "7054                               1     37.59375  112.0  20.000000  59.58398  \n",
      "7055                               1     36.59375  126.0  13.085435  61.43500  \n",
      "7056                               0     39.29688  132.0  13.085435  51.56699  \n",
      "7057                               0     38.50000  114.0  12.000000  60.04898  \n",
      "\n",
      "[7058 rows x 10 columns]\n",
      "Epoch 1/200\n",
      "155/155 [==============================] - 2s 7ms/step - loss: 0.7440 - accuracy: 0.7166 - val_loss: 0.7050 - val_accuracy: 0.6789\n",
      "Epoch 2/200\n",
      "155/155 [==============================] - 1s 8ms/step - loss: 0.5590 - accuracy: 0.8119 - val_loss: 0.5224 - val_accuracy: 0.7819\n",
      "Epoch 3/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.5071 - accuracy: 0.8209 - val_loss: 0.4384 - val_accuracy: 0.8409\n",
      "Epoch 4/200\n",
      "155/155 [==============================] - 1s 7ms/step - loss: 0.4802 - accuracy: 0.8397 - val_loss: 0.4062 - val_accuracy: 0.8706\n",
      "Epoch 5/200\n",
      "155/155 [==============================] - 1s 7ms/step - loss: 0.4517 - accuracy: 0.8538 - val_loss: 0.3889 - val_accuracy: 0.8815\n",
      "Epoch 6/200\n",
      "155/155 [==============================] - 1s 7ms/step - loss: 0.4386 - accuracy: 0.8506 - val_loss: 0.3851 - val_accuracy: 0.8824\n",
      "Epoch 7/200\n",
      "155/155 [==============================] - 1s 8ms/step - loss: 0.4191 - accuracy: 0.8672 - val_loss: 0.3758 - val_accuracy: 0.8834\n",
      "Epoch 8/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.4143 - accuracy: 0.8680 - val_loss: 0.3685 - val_accuracy: 0.8895\n",
      "Epoch 9/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.4045 - accuracy: 0.8711 - val_loss: 0.3623 - val_accuracy: 0.8971\n",
      "Epoch 10/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.4036 - accuracy: 0.8775 - val_loss: 0.3554 - val_accuracy: 0.8985\n",
      "Epoch 11/200\n",
      "155/155 [==============================] - 1s 7ms/step - loss: 0.3825 - accuracy: 0.8806 - val_loss: 0.3578 - val_accuracy: 0.8900\n",
      "Epoch 12/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.3825 - accuracy: 0.8840 - val_loss: 0.3531 - val_accuracy: 0.8952\n",
      "Epoch 13/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.3763 - accuracy: 0.8866 - val_loss: 0.3471 - val_accuracy: 0.8942\n",
      "Epoch 14/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.3679 - accuracy: 0.8848 - val_loss: 0.3473 - val_accuracy: 0.8961\n",
      "Epoch 15/200\n",
      "155/155 [==============================] - 1s 7ms/step - loss: 0.3664 - accuracy: 0.8846 - val_loss: 0.3583 - val_accuracy: 0.8914\n",
      "Epoch 16/200\n",
      "155/155 [==============================] - 1s 7ms/step - loss: 0.3766 - accuracy: 0.8848 - val_loss: 0.3411 - val_accuracy: 0.8952\n",
      "Epoch 17/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.3621 - accuracy: 0.8874 - val_loss: 0.3352 - val_accuracy: 0.9027\n",
      "Epoch 18/200\n",
      "155/155 [==============================] - 1s 7ms/step - loss: 0.3557 - accuracy: 0.8911 - val_loss: 0.3294 - val_accuracy: 0.9060\n",
      "Epoch 19/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.3582 - accuracy: 0.8885 - val_loss: 0.3371 - val_accuracy: 0.8994\n",
      "Epoch 20/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.3517 - accuracy: 0.8877 - val_loss: 0.3316 - val_accuracy: 0.9032\n",
      "Epoch 21/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.3474 - accuracy: 0.8881 - val_loss: 0.3206 - val_accuracy: 0.9070\n",
      "Epoch 22/200\n",
      "155/155 [==============================] - 1s 7ms/step - loss: 0.3478 - accuracy: 0.8913 - val_loss: 0.3157 - val_accuracy: 0.9051\n",
      "Epoch 23/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.3373 - accuracy: 0.8923 - val_loss: 0.3204 - val_accuracy: 0.9046\n",
      "Epoch 24/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.3348 - accuracy: 0.8933 - val_loss: 0.3274 - val_accuracy: 0.8933\n",
      "Epoch 25/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.3357 - accuracy: 0.8933 - val_loss: 0.3067 - val_accuracy: 0.9089\n",
      "Epoch 26/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.3351 - accuracy: 0.8937 - val_loss: 0.3142 - val_accuracy: 0.9056\n",
      "Epoch 27/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.3338 - accuracy: 0.8929 - val_loss: 0.3064 - val_accuracy: 0.9060\n",
      "Epoch 28/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.3214 - accuracy: 0.8976 - val_loss: 0.3017 - val_accuracy: 0.9070\n",
      "Epoch 29/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.3189 - accuracy: 0.8962 - val_loss: 0.3073 - val_accuracy: 0.9051\n",
      "Epoch 30/200\n",
      "155/155 [==============================] - 1s 7ms/step - loss: 0.3226 - accuracy: 0.8935 - val_loss: 0.2965 - val_accuracy: 0.9056\n",
      "Epoch 31/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.3210 - accuracy: 0.8937 - val_loss: 0.3131 - val_accuracy: 0.9023\n",
      "Epoch 32/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.3211 - accuracy: 0.8953 - val_loss: 0.2984 - val_accuracy: 0.9018\n",
      "Epoch 33/200\n",
      "155/155 [==============================] - 1s 8ms/step - loss: 0.3051 - accuracy: 0.8970 - val_loss: 0.2917 - val_accuracy: 0.9108\n",
      "Epoch 34/200\n",
      "155/155 [==============================] - 1s 7ms/step - loss: 0.3156 - accuracy: 0.8953 - val_loss: 0.3053 - val_accuracy: 0.8975\n",
      "Epoch 35/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.3108 - accuracy: 0.9008 - val_loss: 0.2987 - val_accuracy: 0.9032\n",
      "Epoch 36/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.3101 - accuracy: 0.8960 - val_loss: 0.2879 - val_accuracy: 0.9093\n",
      "Epoch 37/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.3123 - accuracy: 0.8947 - val_loss: 0.2953 - val_accuracy: 0.8999\n",
      "Epoch 38/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.3013 - accuracy: 0.8984 - val_loss: 0.2881 - val_accuracy: 0.9084\n",
      "Epoch 39/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.2978 - accuracy: 0.9022 - val_loss: 0.2785 - val_accuracy: 0.9018\n",
      "Epoch 40/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.2950 - accuracy: 0.8980 - val_loss: 0.2819 - val_accuracy: 0.9046\n",
      "Epoch 41/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.2933 - accuracy: 0.9014 - val_loss: 0.2891 - val_accuracy: 0.9056\n",
      "Epoch 42/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.2937 - accuracy: 0.9043 - val_loss: 0.2735 - val_accuracy: 0.9093\n",
      "Epoch 43/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.2940 - accuracy: 0.9028 - val_loss: 0.2823 - val_accuracy: 0.9098\n",
      "Epoch 44/200\n",
      "155/155 [==============================] - 1s 7ms/step - loss: 0.2957 - accuracy: 0.9002 - val_loss: 0.2768 - val_accuracy: 0.9112\n",
      "Epoch 45/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.2888 - accuracy: 0.9045 - val_loss: 0.2759 - val_accuracy: 0.9093\n",
      "Epoch 46/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.2888 - accuracy: 0.9055 - val_loss: 0.2811 - val_accuracy: 0.9023\n",
      "Epoch 47/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.2951 - accuracy: 0.8994 - val_loss: 0.2796 - val_accuracy: 0.8990\n",
      "Epoch 48/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.2837 - accuracy: 0.9024 - val_loss: 0.2682 - val_accuracy: 0.9155\n",
      "Epoch 49/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.2912 - accuracy: 0.9014 - val_loss: 0.2655 - val_accuracy: 0.9122\n",
      "Epoch 50/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.2775 - accuracy: 0.9036 - val_loss: 0.2655 - val_accuracy: 0.9145\n",
      "Epoch 51/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.2828 - accuracy: 0.9045 - val_loss: 0.2794 - val_accuracy: 0.9065\n",
      "Epoch 52/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.2855 - accuracy: 0.8998 - val_loss: 0.2697 - val_accuracy: 0.9079\n",
      "Epoch 53/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.2812 - accuracy: 0.9030 - val_loss: 0.2692 - val_accuracy: 0.9060\n",
      "Epoch 54/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.2904 - accuracy: 0.9002 - val_loss: 0.2758 - val_accuracy: 0.9060\n",
      "Epoch 55/200\n",
      "155/155 [==============================] - 1s 7ms/step - loss: 0.2830 - accuracy: 0.8996 - val_loss: 0.2764 - val_accuracy: 0.9004\n",
      "Epoch 56/200\n",
      "155/155 [==============================] - 1s 7ms/step - loss: 0.2814 - accuracy: 0.9026 - val_loss: 0.2681 - val_accuracy: 0.9103\n",
      "Epoch 57/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.2780 - accuracy: 0.9032 - val_loss: 0.2690 - val_accuracy: 0.9093\n",
      "Epoch 58/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.2760 - accuracy: 0.9077 - val_loss: 0.2649 - val_accuracy: 0.9075\n",
      "Epoch 59/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.2850 - accuracy: 0.9067 - val_loss: 0.2736 - val_accuracy: 0.9079\n",
      "Epoch 60/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.2835 - accuracy: 0.9036 - val_loss: 0.2710 - val_accuracy: 0.9122\n",
      "Epoch 61/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.2697 - accuracy: 0.9095 - val_loss: 0.2729 - val_accuracy: 0.9051\n",
      "Epoch 62/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.2795 - accuracy: 0.9006 - val_loss: 0.2640 - val_accuracy: 0.9108\n",
      "Epoch 63/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.2728 - accuracy: 0.9059 - val_loss: 0.2635 - val_accuracy: 0.9089\n",
      "Epoch 64/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.2686 - accuracy: 0.9097 - val_loss: 0.2709 - val_accuracy: 0.9065\n",
      "Epoch 65/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.2699 - accuracy: 0.9059 - val_loss: 0.2883 - val_accuracy: 0.8942\n",
      "Epoch 66/200\n",
      "155/155 [==============================] - 1s 7ms/step - loss: 0.2744 - accuracy: 0.9016 - val_loss: 0.2697 - val_accuracy: 0.9056\n",
      "Epoch 67/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.2748 - accuracy: 0.9004 - val_loss: 0.2724 - val_accuracy: 0.9075\n",
      "Epoch 68/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.2772 - accuracy: 0.9034 - val_loss: 0.2747 - val_accuracy: 0.9056\n",
      "Epoch 69/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.2702 - accuracy: 0.9119 - val_loss: 0.2776 - val_accuracy: 0.9032\n",
      "Epoch 70/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.2788 - accuracy: 0.9034 - val_loss: 0.2681 - val_accuracy: 0.9089\n",
      "Epoch 71/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.2787 - accuracy: 0.9069 - val_loss: 0.2755 - val_accuracy: 0.9089\n",
      "Epoch 72/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.2868 - accuracy: 0.9053 - val_loss: 0.2678 - val_accuracy: 0.9122\n",
      "Epoch 73/200\n",
      "155/155 [==============================] - 1s 6ms/step - loss: 0.2687 - accuracy: 0.9065 - val_loss: 0.2653 - val_accuracy: 0.9122\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.2635 - accuracy: 0.9089\n",
      "Test Accuracy: 90.89%\n",
      "1/1 [==============================] - 0s 79ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "df = pd.read_csv('test2.csv')\n",
    "\n",
    "if 'Unnamed: 0' in df.columns:\n",
    "    df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Split data into features and target variable\n",
    "X = df.drop('death', axis=1)\n",
    "y = df['death']\n",
    "\n",
    "print(X)\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "import joblib\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(800, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    \n",
    "    tf.keras.layers.Dense(500, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "\n",
    "    tf.keras.layers.Dense(400, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    \n",
    "    tf.keras.layers.Dense(300, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "\n",
    "    tf.keras.layers.Dense(200, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    \n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)  # Adjust learning rate\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "model.compile(opt, \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=200,  # Set a high number since early stopping is implemented\n",
    "                    batch_size=32, \n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Sample prediction\n",
    "sample_data = X_test.iloc[0]  # get the first row from the test set\n",
    "predicted_prob = model.predict(np.array([sample_data]))[0][0]\n",
    "# print(f\"The probability of death is: {predicted_prob * 100:.2f}%\")\n",
    "# predicted_label = (predicted_prob > 0.5).astype(int)\n",
    "# print(f\"Predicted Label: {predicted_label}\")\n",
    "\n",
    "model.save('Mynewmodel.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "bec57339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    def attempt_convert_to_float(column):\n",
    "        try:\n",
    "            return column.astype(float)\n",
    "        except ValueError:  # If conversion fails, return the original column\n",
    "            return column\n",
    "\n",
    "    # Apply the function to each column\n",
    "    # Modify sex column without direct indexing\n",
    "    df.loc[df['sex'].isin([\"male\", \"Male\", \"M\", 1]), 'sex'] = 0\n",
    "    df.loc[~df['sex'].isin([0]), 'sex'] = 1\n",
    "\n",
    "    # Convert to float where possible\n",
    "    df = df.apply(attempt_convert_to_float)\n",
    "\n",
    "    # Impute missing values for float64 columns\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == \"float64\":\n",
    "            df[col].fillna(df[col].mean(), inplace=True)\n",
    "\n",
    "    # Create dummies\n",
    "    df = pd.get_dummies(df, columns=['dnr', 'race', 'primary', 'disability', 'income', 'extraprimary', 'cancer'], drop_first=True)\n",
    "\n",
    "    # Ensure columns are actually dropped\n",
    "    df = df.drop(['dose', 'pdeath'], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "be5affdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      timeknown  bloodchem4        urine     glucose  administratorcost  \\\n",
      "0           4.0   12.000000  5360.000000  157.000000           3525.000   \n",
      "1         467.0   10.000000  2570.000000  271.000000          43200.000   \n",
      "2         533.0   28.000000  1690.000000  117.000000           5894.000   \n",
      "3          68.0   31.790697  2197.483816  159.695613          16717.000   \n",
      "4        1605.0   31.790697  2197.483816  159.695613          10151.000   \n",
      "...         ...         ...          ...         ...                ...   \n",
      "7053      841.0   12.000000  1830.000000  159.695613          43891.000   \n",
      "7054      258.0   39.000000  1130.000000  139.000000         106109.000   \n",
      "7055     1325.0   31.790697  2197.483816  159.695613          21128.000   \n",
      "7056        4.0   31.790697  2197.483816  159.695613          22312.328   \n",
      "7057       14.0   17.000000  1840.000000   96.000000           5874.000   \n",
      "\n",
      "      disability_no(M2 and SIP pres)  temperature  heart  education       age  \n",
      "0                                  0     35.59375  103.0  20.000000  76.56396  \n",
      "1                                  0     39.00000   50.0  16.000000  63.33499  \n",
      "2                                  1     38.19531   50.0   5.000000  70.52698  \n",
      "3                                  0     37.59375   80.0  12.000000  55.31799  \n",
      "4                                  1     35.69531  114.0   2.000000  67.06598  \n",
      "...                              ...          ...    ...        ...       ...  \n",
      "7053                               0     38.00000  150.0  17.000000  88.91199  \n",
      "7054                               1     37.59375  112.0  20.000000  59.58398  \n",
      "7055                               1     36.59375  126.0  13.085435  61.43500  \n",
      "7056                               0     39.29688  132.0  13.085435  51.56699  \n",
      "7057                               0     38.50000  114.0  12.000000  60.04898  \n",
      "\n",
      "[7058 rows x 10 columns]\n",
      "221/221 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[162], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m     prob \u001b[38;5;241m=\u001b[39m solution\u001b[38;5;241m.\u001b[39mcalculate_death_prob(df)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prob[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mq1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[162], line 24\u001b[0m, in \u001b[0;36mq1\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m solution \u001b[38;5;241m=\u001b[39m Solution()\n\u001b[1;32m     23\u001b[0m prob \u001b[38;5;241m=\u001b[39m solution\u001b[38;5;241m.\u001b[39mcalculate_death_prob(df)\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprob\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "class Solution:\n",
    "    def __init__(self):\n",
    "        self.model = tf.keras.models.load_model('Mynewmodel.h5')\n",
    "        self.scaler = joblib.load(\"scaler.pkl\") # You may want to save and load this as well if you're running the preprocessing on different datasets\n",
    "        self.significant_features = ['timeknown','bloodchem4','urine','glucose','administratorcost','disability_no(M2 and SIP pres)','temperature','heart','education','age'] # Populate this with the significant features if needed\n",
    "\n",
    "    def calculate_death_prob(self, df):\n",
    "        df = preprocess_data(df)\n",
    "        # Only use significant features if needed\n",
    "        X_new = df[self.significant_features]\n",
    "        print(X_new)\n",
    "        X_new_scaled = self.scaler.transform(X_new)\n",
    "        prediction = self.model.predict(X_new_scaled)\n",
    "        return float(prediction[0][0])\n",
    "\n",
    "def q1():\n",
    "    # data = request.json\n",
    "    # df = pd.DataFrame(data)\n",
    "    df = pd.read_csv('TD_HOSPITAL_TRAIN.csv')\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "    solution = Solution()\n",
    "    prob = solution.calculate_death_prob(df)\n",
    "    return \n",
    "\n",
    "\n",
    "print(q1())\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a82818eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- administratorcost\n- age\n- blood\n- bloodchem1\n- bloodchem2\n- ...\nFeature names seen at fit time, yet now missing:\n- Unnamed: 0\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     prob \u001b[38;5;241m=\u001b[39m solution\u001b[38;5;241m.\u001b[39mcalculate_death_prob(df)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jsonify({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprobability\u001b[39m\u001b[38;5;124m\"\u001b[39m: prob})\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mq1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# if __name__ == \"__main__\":\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#     app.run(host=\"0.0.0.0\", port=5555)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[86], line 9\u001b[0m, in \u001b[0;36mq1\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTD_HOSPITAL_TRAIN.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m solution \u001b[38;5;241m=\u001b[39m Solution()\n\u001b[0;32m----> 9\u001b[0m prob \u001b[38;5;241m=\u001b[39m \u001b[43msolution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_death_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jsonify({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprobability\u001b[39m\u001b[38;5;124m\"\u001b[39m: prob})\n",
      "Cell \u001b[0;32mIn[85], line 12\u001b[0m, in \u001b[0;36mSolution.calculate_death_prob\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Only use significant features if needed\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# X_new = df[self.significant_features]\u001b[39;00m\n\u001b[1;32m     11\u001b[0m X_new \u001b[38;5;241m=\u001b[39m df\n\u001b[0;32m---> 12\u001b[0m X_new_scaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_new\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(X_new_scaled)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(prediction[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 157\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    158\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    160\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    162\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    163\u001b[0m         )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:1006\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1003\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m   1005\u001b[0m copy \u001b[39m=\u001b[39m copy \u001b[39mif\u001b[39;00m copy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy\n\u001b[0;32m-> 1006\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m   1007\u001b[0m     X,\n\u001b[1;32m   1008\u001b[0m     reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1009\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1010\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m   1011\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[1;32m   1012\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1013\u001b[0m )\n\u001b[1;32m   1015\u001b[0m \u001b[39mif\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(X):\n\u001b[1;32m   1016\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_mean:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:580\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate_data\u001b[39m(\n\u001b[1;32m    510\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    511\u001b[0m     X\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mno_validation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params,\n\u001b[1;32m    517\u001b[0m ):\n\u001b[1;32m    518\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \n\u001b[1;32m    520\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[39m        validated.\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 580\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_feature_names(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[1;32m    582\u001b[0m     \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_tags()[\u001b[39m\"\u001b[39m\u001b[39mrequires_y\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    583\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    584\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m estimator \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    585\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mrequires y to be passed, but the target y is None.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    586\u001b[0m         )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:507\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m missing_names \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m unexpected_names:\n\u001b[1;32m    503\u001b[0m     message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    504\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m     )\n\u001b[0;32m--> 507\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(message)\n",
      "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- administratorcost\n- age\n- blood\n- bloodchem1\n- bloodchem2\n- ...\nFeature names seen at fit time, yet now missing:\n- Unnamed: 0\n"
     ]
    }
   ],
   "source": [
    "# app = Flask(__name__)\n",
    "\n",
    "# @app.route(\"/death_probability\", methods=[\"POST\"])\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     app.run(host=\"0.0.0.0\", port=5555)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
